ChannelHandler 他的实现负责接收并相应事件通知，可以是使用ChannelInboundHandlerAdapter类，提供了默认的
ChannelInboundHandler的实现
channelRead 对每个传入的消息都要调用
channelReadComplete 通知ChannelInboundHandler最后一个对channelRead()的调用时当前批量读取的最后一条消息。
exceptionCaught 在读取期间，有一场抛出时会调用

ChannelInitializer 一个CHannleHandler的实例，
当一个新的连接被接受时，会产生一个新的Channel将会被创建，而CHannelInitializr将会把一个你的
自定义个CHannelHandler添加到这个Channel的CHannelPipeline中，这个CHannelHandler会受到有关入栈消息的通知。
channel	NIOServerSocketChannel NioSocketChannel 
ChannelFuture 异步的future

EventLoop EventLoopGroup 使用EventLoop处理IO时间
一个EventLoop在生命周期内之和一个Thread绑定
Chaneel 所有属于通过一个Channel的操作都被保证其将以调用的顺序执行
ChannelPipeline 提供CHannelHandler链的容器。Channel创建时会被自动的分配专属的ChannelPipeline，
	1，ChannelInitializer 的实现被注册到了BootStrap中
	2，ChannelInitializer.initChannel 方法被调用，把ChannelHandler添加到ChannelPipeline链中
	3，ChannelInitializer将他自己从ChannelPipeline中移除,因为ChannelInitialixer也是一个CHannelHandler，需要移除。
channelHandler 处理逻辑，CHannelHandler接收事件，执行他们所实现的处理逻辑，并将数据传给下一个CHannelHandler，
	执行顺序是由他们被添加的顺序所决定，
	ChannelInoundHandler 接收入站事件和数据，这些数据随后将会被你的应用程序的业务逻辑所处理。
		SimpleChannelInboundHandlerAdapter
	ChannelOutboundHandler	出站事件和数据，数据的出站运动（即正在被写的数据），数据将从ChannelOutboundHandler
	链的尾端开始流动，知道它到达链的头部为止，在这之后，数据将会到达网络传输层，通常情况下会触发一个写操作。
	ChannelHandlerAdapter
	ChannelDuplexHandler
	ChannelHandler用途，1将数据从一种格式转为另外一种格式
		2，提供异常的通知，
		3，提供CHannel变为活动的或者非活动的通知，
		4，提供当Channel注册到EventLoop或从EventLoop中注销的通知
		5，提供有关用户自定义事件的通知
CHannelHandlerContext 代表CHannelHandler和ChannelPipeline之间的绑定。虽然可以获取Channel
	但主要功能还是用来写出站数据
Netty有两种消息发送机制，
	1，直接写入CHannel中，导致消息从ChannelPipeline尾端开始流动
	2，写入ChannelHandlerContext ctx中，导致消息从ChannelPipeline中的下一个CHannelHandler开始流动，ctx.fiteChannelRead
网络数据总是一系列的字节
ByteToMessageDecoder MessageToByteEncoder
BootStrap，客户端连接到远程服务器，使用一个EventLoopGroup
ServerBootStrap	，服务端，绑定本地连接，使用2个EventLoopGroup，可以使同一个实例，
	parentGroup只包含ServerChannel，代表服务器自身的已经绑定到某个本地端口的正在监听的套接字，
	parentGroup中的EventLoop负责为传入的连接请求创建Channel，EventLoopGroup负责分配这个EventLoop
	childGroup包含所有已创建的用来处理传入客户端连接的Channel

Netty的Channel是线程安全的。消息会被保证按顺序发送
零拷贝 是一种快速搞笑的将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间
Epoll linux的本地非阻塞传输
Netty 使用Epoll，linux环境，高负载情况下，可能由于JDK NIO
	只需将NioEventLoopGroup替换为EpollEventLoopGroup，NIOServerSocketChannel替换为
	EpollServerSocketChannel即可。
非阻塞代码库或一个常规的起点	NIO 或在Linux上的epoll
阻塞代码库			OIO
在同一个JVM内部的通信		Local，Default
测试ChannelHandler的实现	Embedded

ByteBuf readIndex,writeIndex,capacity, get,set方法不会触发readIndex，writeIndex的改变。	
	read,write方法改变readIndex，writeIndex，也可以手动调用readIndex(),writeIndex()方法修改这两个值。
directByteBuf，JVM堆外内存，主要是为了避免在每次调用本地I/O 操作之前（或者之后）将缓冲区的内容复
制到一个中间缓冲区（或者从中间缓冲区把内容复制到缓冲区）。不回收垃圾回收影响，当然分配和回收较为昂贵。
	回收需要full GC来回收。。。
支撑ByteBuffy  有byte[]支撑，JVM堆空间中
复合缓冲区 CompositByteBuf
	
0 >>> readIndex 可丢弃字节,调用discardReadBytes()方法，可以丢弃
他们并回收空间，只是移动可以读取的字节，writeIndex，对于可写字段的内容并没有任何保证，并不会对可写入字节区域
的数据进行擦除。discardReadBytes方法可能会导致内存复制，因为可读字节必须
被移动到缓冲区开始的位置，只有在真正需要的时候采取调用这个方法。
调用clear() 只是把readIndex,writeIndex置为0，所以相对于discardReadBytes方法轻量得多。

派生缓冲区：
	duplicate()	
	slice()	
	Unpooled.unmodifiableBuffer()
	order(ByteOrder)
	readSlice(int)
	返回新的ByteBuf，具有自己的读写索引和标记索引，内部存储是共享的，使得
	派生索引创建低廉，但是修改了内容的话，同样也会修改其对应的源实例，
复制，如果要用一个缓冲区的真实副本，使用copy方法，返回拥有独立数据的

Channel生命周期：
 	ChannelUnregistered：Channel已经被创建，但还未注册到EventLoop
	ChannelRegistered：Channel已经被注册到了EventLoop
	ChannelActive:Channel处于活动状态(已经连接到其他的远程节点)	,可以发送接收数据了
	ChannelInactive:Channel没有连接到远程节点。
	
	ChannelRegister -> ChannelActive -> ChannelInActive-> CHannelUnregistered
	
ChannelHandler的生命周期，在CHannelHandler被
	添加到ChannelPipeline或者从ChannelPipeline中移除会调用该
	这些操作，接收一个ChannelHandlerContext参数
	HandlerAdded 当把CHannelHandler添加到ChannelPipeline中
	HandlerRemove 当从ChannelPileline中移除CHannelHandler时
	exceptionCaught 当处理过程中在ChannelPipeline中有错误时被调用。

ChannelInboundHandler	
	ChannelRegistered 当Channel已经注册到他的EventLoop，并能够处理IO是被调用
	ChannelUnregistered 当Channel从他的EventLoop注销并无法处理任何IO是被调用
	ChannelActive 当Channel处于活动状态时被调用，Channel已经连接/绑定并且已经就绪
	ChannelInActive 当Channel离开活动状态并且不在连接他的远程节点是被调用
	ChannelRead 当从Channel中读取数据是被调用
	ChannelReadComplete 当Channel上的一个读操作完成时被调用
	ChannelWritabilityChanged 当Channel的可写状态发生改变时被调用，用户可以确保写操作不会完成的太快，
	（用于避免OutofMemoryError）或者可以再Channel变为再次可写时恢复写入，可以通过Channel.isWritable方法
	来检测Channel的可写性，
	userEventTriggered 当ChannelInboundHandler.fireUserEventTriggered()方法
	被调用时调用，因为一个POJO被传经了CHannelPipeline
	
	当某个ChannelInboundHandler的实现重写ChannelRead()方法时，他将负责
	现实的释放与池化的ByteBuf实例相关的内存，使用ReferenceCountUtil.release();
	使用SimpleChannelInboundHandler无需手动释放。

	ChannelOutboundHandler 出站操作和数据将由ChannelOutboundHandler处理，其中一个强大的功能
	是可以按需推迟操作或者事件，这使得可以通过一些复杂的方法来处理请求，例如，如果到远程节点的写入被暂停了，
	那么你可以推迟冲刷操作并在稍后继续。
	bind(channelHandlerContext,SocketAddress,ChannelPromise) 当请求将Channel绑定到本地地址是调用
	connect(ChannelHandlerContext,SocketAddress,SocketAddress,ChannelPromise) 当请求将Channel连接到远程节点时调用，
	disconnect(CHannelHandlerContext,ChannelPromise) 当请求将Channel从远程节点断开时被调用
	close()请求关闭时调用
	deregister() 当请求将CHannel从他的EventLoop注销时被调用，
	read() 当请求从Channel读取更多的数据时被调用。
	flush() 当请求通过CHannel将入队数据冲刷到远程节点时被调用
	write() 当请求通过CHannel将数据写入到远程节点时被调用,通过ChannelContextHandler写入不会调用。。。

池化资源回收：如果一个消息被消费或者丢弃了，并且没有传递给ChannelPipeline中的下一个ChannelHandler，那么用户就有
责任调用ReferenceCountUtil.release()。如果消息到达了实际的传输层，那么他被写入是或者Channel关闭时，都将被自动释放。
对于写的数据，每次写完之后都会release释放资源。调用write会释放资源，Channel关闭时也会再次释放资源，关闭Channel时关闭的却是DirectBuf，并不是我们申请的那一个。
对于读到的ByteBuf数据，要在最后的ChannelInboundHandler中调用ReferenceCountHandler.release方法，确定释放了资源。
	
ChannelOutboundHandler的write方法,如果消息被消费或者丢弃了,而且并没有传递给ChannelPipeline中下一个ChannelOutboundHandler那么就需要调用ReferenceCountUtil
和ChannelPromise的setSuccess之类的方法处理。

ChannelPipeline：每一个新建的ChannelPipeline都会被
	分配一个新的ChannelPipeline，Channel的生命周期内既不能附加另外一个ChannelPipeline，也不能分离器当前的
	
ChannelHandlerContext使得ChannelHandler能够和他的ChannelPipeline以及其他的CHannelHandler交互，
ChannelHandler可以通知其所属的ChannelPipeline中的下一个CHannelHandler。甚至可以动态地修改他所属的ChannelPipeline，	
	
add 添加CHannelHandler
remove	移除ChannelHandler
replace	替换
		
ChannelHandler的执行和阻塞
	通常ChannelPipeline中的每个ChannelHand而都是通过他的EventLoop来处理传递给他的事件的，所以不要阻塞这个线程，
	不要阻塞这个线程!不要阻塞这个线程！但是有时会有可能与阻塞API交互，ChannelPipeline有接收一个EventExecutorGroup的add的方法，
	如果一个事件被传递给一个自定义的EventExecutorGroup，他将被包含在这个EventExecutorGroup中的某个EventExecutor处理，从而
	被Channel本身的EventLoop中移除，对于这种用例，Netty提供了一个叫DefaultEventExecutorGroup的默认实现，
	
	关注这里的定时任务，如果没有传递DefaultEventExecutorGroup ,那么就是用的是 EventLoopGroup也就是IO
	线程，客户端的话应该是没什么问题，使用默认的eventLoopGroup也是没有问题的
	,如果是服务端，而且任务是长时间执行的话，应该需要 传递一个DefaultEventLoopGroup，
	避免阻塞IO线程。长时间阻塞IO线程导致服务器端处理能力下降。
	
	ChannelPipeline用于访问ChannelHanlder的操作
	get 通过类型或者名称返回CHannelHandler
	context 返回和CHannelHandler绑定的ChannelHandlerContext
	names 返回ChannelPipeline中所有ChannelHandler的名称

ChannelPipeline的入站操作	
	FireChannelXX 调用ChannelPipeline中的下一个ChannelInboundHandler的ChannelXX方法

ChannelPipeline的出站操作
	bind -> 将Channel绑定到下一个本地地址，将调用ChannelPipeline中的下一个ChannelOutboundHandler的bind方法。
	connect	->	将Channel连接到一个远程地址，将调用ChannelPipeline中下一个ChannelOutboundHandlerHandler的connect方法
	disconnect ->	将Channel断开连接
	close -> 将Channel关闭
	deregister -> 将Channel从它先前分配的EventExecutor(EventLoop)中注销
	flush 冲刷Channel所有挂起的写入
	write -> 将消息写入Channel，这并不会将消息写入底层的Socket，而只是会把数据放入队列中，
		需要调用flush 或者 writeAndFlush方法。
	writeAndFlush 先write然后flush
	read -> 请求从Channel中读取更多的数据。	
	
	ChannelPipeline保存了与Channel相关联的CHannelHandler
	ChannelPipeline可以根据需要，通过添加或者删除Channelhandler来动态修改
	ChannelPipeline有着丰富的API，以响应入站和出站事件

ChannelHandlerContext 
	alloc 返回和实例相关联的Channel所配置ByteBufferAllocator
	bind 绑定到给定的SocketAddress,并返回ChannelFuture
	close 关闭Channel,并返回ChannelFuture
	connect 连接给定的SocketAddress，并返回ChannelFuture
	disConnect 从远程节点断开，并返回ChannelFuture。
	executor 返回调度事件的EventExecutor
	name 返回这个实例的唯一名称
	pipeline 返回这个实例所关联的ChannelPipeline
	Read
	Handler 返回绑定到这个实例的ChannelHandler
	
	ChannelHandlerContext和ChannelHandler之间的关联（绑定）是永远
	不会改变的，所以缓存对他的引用时安全的，每一个ChannelHandler都有自己的ChannelHandlerContext。
	
	ChannelHandlerContext的方法将会产生更短的事件流，应该尽可能的利用这个
	特性获得最大的性能，
	
	使用Channel的write方法会导致时间从尾端到头部流经ChannelPipeline

入站异常
如果发生异常，而且没有CHannelHandler处理那么就会抛出以下警告
2017-12-12 12:34:52 [nioEventLoopGroup-3-1] 
[i.n.channel.DefaultChannelPipeline.warn:151] WARN  - 
An exceptionCaught() event was fired, and it reached at the tail of the pipeline. 
It usually means the last handler in the pipeline did not handle the exception.

出站异常
每个出站操作都返回一个ChannelFuture，祖册到CHannelFuture的ChannleFutureListener将在完成时被通知高操作是成功
还是出错。
几乎所有的CHannelOutboundHandler的方法都会传入一个CHannelPromise的实力，作为ChannelFuture的子类，ChannelPromise
可以被分配用于异步通知的监听器，但是ChannelPromise还提供了立即通知的可写方法

setSuccess setFailure

永远不要将一个长时间运行的任务放入到执行队列中，因为它将阻塞需要在同一线程上执行的任何其他任务。
EventLoopGroup 负责为每个新创建的Channel分配一个EventLoop，一旦一个Channel被分配给了一个EventLoop，他将在他的整个生命周期
都是用这个EventLoop(以及相关联的Thread)

bootstrap 实现cloneable，但是他的拷贝是一个浅拷贝，所有的克隆的Bootstrap都会使用同一个EventLoopGroup，
这在一些情况下是可用的

EmbeddedChannel
	writeInbound() 将入栈消息写到EmbeddedChannel中，如果可以通过readInbound()方法从EmbeddedChannel中读取数据，返回true
	readInbound()	从EmbeddedChannel中读取一个入站消息，任何返回的数据都穿越了整个ChannelPipeline，如果没有任何可供读取的数据，则返回null
	writeOutbound 	将出站消息写到EmbeddedChannel中。
	readOutbound
	finish 将EmbeddedCHannel标记为完成，并且如果有可被读取得入站数据或者出站数据，则返回true，这个方法还会调用EmbeddedCHannel上的close方法
	
FixedLengthFrameDecoder
	
ByteToMessageDecoder
	decode 必须实现的唯一抽象方法。传入包含数据的bytebuf，对这个方法的调用会重复进行，以确保没有新的元素被添加到List，如果List不为空，那么他的内容会被传递给ChannelPipeline中的下一个CHannelInboundHandler
	
	decodeLast 默认实现只是简单调用decode方法，当Channel的状态变为非活动时，这个方法将会被调用一次，可以重写该方法以提供特殊的处理。

编解码器中的引用计数
	对于编解码器而言，一旦消息被编码或者被解码，就会被referenceCountUtil.release(message)调用释放，如果要保留引用以便稍后使用，那么要调用
	ReferenceCountUtil.retain(message)方法，这将会增加引用计数，从而防止该消息被释放。
	
ReplayingDecoder用异常捕获机制来做到对读取数据长度的校验，
	每次ReplayingDecoderByteBuf读取不到合适的数据就会抛出Signal，然后被捕获，忽略，继续读取，感觉略低效。
	
如果使用ByteToMessageDecoder不会引入太多复杂性，就使用，否则用ReplayingDecoder	
lineBasedFrameDecoder使用行尾控制符来解析消息数据
HTTPObjectDecoder http数据解码器
io.netty.handler.codec

MessageToMessageDecoder
	把消息从一种格式转换为另一种格式。
	
ToLongFrameException 不能让解码器缓存大量数据，以至于耗尽可用的内存。
	
MessageToByteEncoder
	encode	

解码器decoder通常要在关闭之后产生最后一个消息，因此也就有了decodeLast，这显然不适合编码器的场景，

MessageToMessageEncoder 每个通过write方法写入的消息都会被传递给encode方法，

ByteToMessageCodec	结合ByteToMessageDecoder以及MessageToByteEncoder
	先解码，然后再次编码。
	decode 
	decodeLast
	encode
MessageToMessageCodec<INBOUND_IN,OUTBOUND_IN) INBOUND_IN 网络传输类型，OUT_BOUND内部使用类型	
	decode	INBOUND -> OUTBOUND
	encode	OUTBOUNT -> INBOUND

ombinedChannelDuplexHandler 组合起来的编解码器


Http请求的第一部分包含了Http请求信息，第二部分包含了一个或多个HTTPContent信息，第三部分包含Http请求结束的信息(LastHttpContent标记了该Http请求结束)
HTTPRequest -> HttpContent .... -> LastHttpContent
HttpResponse -> HttpContent ... -> LastHttpContent

FullHTTPRequest和FullHTTPResponse 代表了完整的请求和相应。所有类型的Http消息都实现了HTTPObject接口。

HttpRequestEncoder 将HTTPRequest,HttpContent,LastHttpContent 编码为字节
HttpResponseEncoder 将HTTPREsponse，HTTPContent，LastHTTPContent编码为字节
HttpRequestEncoder 将字节码解码为HTTPRequest，HTTPContent，LastHTTPContent
HttpResponseDecoder 将字节码解码为HttpResponse，HTTPContent，LastHTTPContent

聚合Http消息，Http消息可能有很多部分，所以需要聚合已形成一个完整的消息。
由于消息需要被缓冲，直到可以转发一个完整的消息提供给下一个ChannelInboundHander，所以这个操作
有轻微的开销。

Http压缩
	当时用Http时，建议开启压缩功能以尽可能多的减少传输数据的大小，虽然压缩会带来一些cpu上的开销，但通常来说都是个好主意、
	Netty提供了CHannelHandler实现，同时支持gzip和delate编码

BinaryWebSocketFrame	 数据帧：二进制数据
TextWebSocketFrame		数据帧：文本数据	
ContinuationWebSocketFrame	数据帧：属于上一个BInaryWebSocketFrame或者TextWebSocketFrame的文本或者二进制数据
CloseWebSocketFrame		控制帧 一个close请求，挂你的状态码以及关闭的原因
PingWebSocketFrame		控制帧，请求一个PongWEbSOcketFrame
PongWebSocketFrame		控制帧：对PingWebSocketFrame	请求的响应

IdleStateHandler	当空闲时间太长，将会触发一个IdelStateEvent事件，然后可以在CHannelInboundHandlerHandler中重写userEventTriggered方法来处理该IdelStateEvent事件
ReadTimeoutHandler	如果在指定的时间间隔内没有收到任何的入站数据，则抛出一个ReadTimeoutException并关闭对应的Channel，可以通过重写你的CHannelHandler中的exceptionCaught方法来检测ReadTimeoutException
WriteTimeoutHandler 如果指定的时间间隔内并没有任何出站数据，则抛出一个writeTimeoutException并关闭对应的Channel，可以通过重写CHannelHandler的exceptionCaught方法来检测该WriteTimeoutException

使用分隔符的协议解码器。
DelimiterBaseFrameDecoder 使用任何由用户提供的分隔符来提取数据帧的通用解码器
LineBasedFrameDecoder	提取由行尾(\r\n ,\n)分隔的解码器，比DelimiterBasedFrameDecoder更快

基于长度的协议
	基于长度的协议通过将他的长度编码到帧头部来定义帧，而不是使用特殊的分隔符来标记
	
	FixedLengthFrameDecoder 用于提取指定长度的定长帧
	LengthFieldBasedFrameDecoder 用于处理变长帧 	根据编码帧头部中的长度值提取帧，该字段的偏移量以及长度在构造函数中指定。
	
	长度在帧中偏移量及偏移量长度

写大数据量，由于写操作是非阻塞的，所以即使没有写出所有数据，写操作也会在完成时返回并通知CHannelFuture，这时，如果依然不停写入，就有内存耗尽的风险。
	所以在写大型数据时，需要准备好处理到远程节点的连接是慢速连接的情况。
	使用FileRegion接口的实现，通过支持零拷贝的文件传输的Channel来发送文件区域。
	
使用FileRegion的情况只适合于文件内容的直接传输，不包括应用程序对数据进行任何处理，如果有需要处理的情况(如使用SSL加密)
	可以使用ChunkedWriteHandler，ChunkedFile，它支持异步写大型数据流，而又不会导致大量内存损耗。

ChunkedFile 从文件中逐块获取数据，当你的平台不支持零拷贝或者你需要转换数据时使用
ChunkedNioFile  和ChunkedFile类似，只是他是用了FileChannel。
ChunkedStream 	从InputStream中逐块的传输内容
ChunkedNioStream  从ReadableByteChannel 中逐块传输内容。	
	
序列化
	CompatibleObjectDecoder 和使用JDK序列化的非Netty的远程节点进行相互操作的解码器
	CompatibleObjectENcoder 和使用JDK序列化的 编码器
	ObjectDecoder 构建于JDK序列化之上的使用自定义的序列化的解码器，当没有其他的外部依赖时，这个提供了速度上的改进，否则其他的序列化实现更加可取
	ObjectEncoder 

JBoss Marshaling	
	compatibleMarshallingDecoder 
	compatibleMarshallingEncoder 与只是用JDK序列化的远程节点兼容
	
	MarshallingDecoder
	MarshallingEncoder 适用于JBoss Marshalling的节点，必须一起使用。
	
Protobuf
	ProtobufDecoder		使用protobuf对消息解码
	ProtobufEncoder
	ProtobufVarint32FrameDecoder	根据消息中的Google Protocol Buffers 的Base 128 Varints 整形长度字段动态的分割所接受的ByteBuf,接收时用来分割接收到的信息，
	ProtobufVarint32LengthFieldPrepender	向ByteBuf前追加一个Google Protocal BUffers的Base 128Varints整形的长度字符串，发送时用来添加分割使用的信息

WebSocket帧 WebSocket以帧的方式传输数据，每一帧代表消息的一部分，一个完整的消息可能会包含许多帧。
	BinaryWebSocketFrame	 包含二进制数据
	TextWebSocketFrame		包含文本数据
	ContinuationWebSocketFrame	包含属于上一个BinaryWebSocketFrame或者TextWebSocjetFrame的文本数据或者二进制数据
	CloseWebSocketFrame		表示一个CLOSE请求，包含一个关闭的状态码和关闭的原因。
	PingWebSocketFrame		请求传输一个PongWebSocketFrame
	PongWebSocketFrame		作为一个对PingWebSocketFrame的响应被发送。
	
	HttpServerCodec 	将字节码解为HTTPRequest，HTTPContent和lastHTTPContent，并将HTTPResponse，HTTPContent，LastHTTPContent编码为字节。
	ChunkedWriteHandler	写入大码流文件，而不会出现OutOfMemory
	httpObjectAggregator	讲一个HttpMessage和跟随他的多个HTTPContent，LastHTTPContent聚合为单个FullHTTPRequest，FullHTTPResponse，安装这个
		Channelhandler之后，ChannelPipeline中的下一个ChannelHandler将只会收到完整的Http请求
		
	WebSocketServerProtocolHandler	按照WebSocket规范的要求，处理WebSocket升级握手，PingWebSocjetFrame，PongWebSocketFrame，和CloseWebSocketFrame.
		Netty的WebSocketServerprotocolhandler处理了所有委托管理的WebSocket帧类型以及升级握手本身，如果握手成功，那么所需的ChannelHandler将会被添加到ChannelPipeline中，而那些不再需要的ChannelHandler将会被移除
		当WebSocket升级完成之后，WebSocketServerProtocolhandler将会把HTTPRequestDecoder替换为WebSocketFrameDecoder，HttpResponseEncoder替换为WebSocketFrameEncoder，为了性能的最大化，将移除任何卜再被WebSocket
		所需要的ChannelHandler，（包括httpObjectAggregator，Httprequesthandler）
	##TextWebSocketFramehandler	处理TextWebSocketFrame和握手事件。
	
UDP
	AddressedEnvelope<M,A extends SocketAddress>	定义一个消息，其包装了另一个消息并带有发送者和接受者地址，	M是消息类型，A是地址类型
	DefaultAddressedEnvelope	提供了AddressedEnvelope的默认实现
	DatagramPacket extends DefalutAddressedEnvelope implements ByteBufHolder	扩展了DefaultAddressedEnvelope以使用ByteBuf作为消息数据容器。
	DatagramChannel 扩展了Netty的Channel抽象以支持UDP的多播组管理
	NioDatagramChannel 定义了一个能够发送和接受AddressedEnvelope消息的Channel
	
	DtagramPacket是一个简单的消息容器，DatagrameChannel实现用它来和远程节点通信。
	包含了接受者和可选的发送者的地址，以及消息的有效负载本身。

ChannelOptin
SO_RECVBUF和SO_SNDBUF 通常建议值为128K或者256K
SO_TCPNODELAY：NAGLE算法通过将缓冲区内的小封包自动组成一个大的封包，组织大量小包发送阻塞网络，从而提高网络效率，但是对于延时敏感的场景需要关闭这项优化，true 关闭优化，有数据就发。
软终端：如果LINUX内
SO_BACKLOG 用于构建服务器端ServerSocket，标识当服务器请求处理线程全满，用于临时存放已完成3次握手请求的队列的最大长度。
SO_KEEPALIVE	启动心跳机制。如果在两个小时内没有数据通信，TCP会自动发送一个活动探测数据报文。



ChannelInboundHandlerAdapter需要自己释放接收到的ByteBuffer。****
通常作如下处理,一定不要忘记释放Bytebuf，否则可能会造成内存泄漏
@override
public void channedRead(ChannelHandlerContext ctx,Object msg){

	try{
	
	}finally{
		ReferenceCountUtil.release(msg);
	}
}


传递给ByteToMessageDecoder和MessageToByteEncoder的 ByteBuf 无需我们自己释放，会被Netty自动释放掉，
	ByteToMesageDecoder的释放代码在channelRead finally中，MessageToByteEncoder的在write方法中

Netty中 A线程申请的内存一定要有A线程释放，否则有可能会存在内存泄漏的问题
	一定不要跨线程释放。

IdleStateHandler会启动ReaderIdleTimeTask，WriteidleTimeoutTask和AllIdleTimeoutTask，都会被加入到NioVentLoopGroup中，
比单独的ReadTimeoutHandler要消耗性能。readIdle,writeIeld,allIdle在设置为0的时候，不起作用。

linux 优化
	linux 内核最大文件句柄数，默认1024，可以使用
	 ulimit -a 查看 open files
	 vi /etc/security/limits.conf 
担心 close_WAIT 
	close_WAIT 状态下并不会释放句柄和内存资源，如果积压过多，可能会导致内存溢出，too many open files等问题。

不要再IO线程上处理业务(心跳发送和检测除外)	
	IO线程推荐值[cpu核数+1，cpu核数*2]
	当有复杂逻辑阻塞IO线程是，会影响对其他链路的读写操作。		 
设计要点2：在I/O线程上执行自定义Task要当心。Netty的I/O处理线程NioEventLoop支持两种自定义Task的执行：

普通的Runnable: 通过调用NioEventLoop的execute(Runnable task)方法执行；
定时任务ScheduledFutureTask:通过调用NioEventLoop的schedule(Runnable command, long delay, TimeUnit unit)系列接口执行。

尽量不要使用NIOEventLoop中的线程指定自定义的定时任务(心跳除外)。
可以添加DefaultEventExecutorGroup执行。

设计要点3：IdleStateHandler使用要当心。很多用户会使用IdleStateHandler做心跳发送和检测，这种用法值得提倡。相比于自己启定时任务发送心跳，
这种方式更高效。但是在实际开发中需要注意的是，在心跳的业务逻辑处理中，无论是正常还是异常场景，处理时延要可控，防止时延不可控导致的NioEventLoop被意外阻塞
例如，心跳超时或者发生I/O异常时，业务调用Email发送接口告警，由于Email服务端处理超时，导致邮件发送客户端被阻塞，级联引起IdleStateHandler的AllIdleTimeoutTask任务被阻塞，
最终NioEventLoop多路复用器上其它的链路读写被阻塞。

合理的心跳周期	
	微信心跳周期300s，或许180s不错。
	
使用IdlestateHandler处理心跳，
public void userEventTriggered({@link ChannelHandlerContext} ctx, {@link Object} evt) throws {@link Exception} {
          if (evt instanceof {@link IdleStateEvent}} {
              //心跳处理
              ctx.writeAndFlush(ping);
          }
      }

ByteBuf支持动态容量调整，对于接受缓中区得内存分配，Netty提供了两种。
	FixedRecvByteBufAllocator：固定长度的接受缓冲区分配器，分配的大小长度固定，并不会根据实际数据的大小动态收缩。
	AdaptiveRecvByteBufAllocator：容量动态调整的接受缓冲区分配器。他会根据之前Channel接收到的数据包大小进行计算，
	如果连续填充满接收缓冲区的可写空间，就动态扩展容量，如果连续两次节后到的数据都小于指定值，则收缩当前容量，以节约内存。
	
	注：默认使用的就是AdaptiveRecvByteBufAllocator(64,1024,65536) 可以根据自己的需要调节大小。
	大小建议为消息的平均值。

netty 4.17已经默认使用内存池了。

	Netty的IO线程不能执行时间不可控的操作，例如数据库访问，发送email等
	记录日志Logger.error在某些情况下也会阻塞IO，导致高并发下的奇怪问题。
	
NioEventLoop中的processSelectedKey 方法，可以看到netty对java nio的封装。
Netty 中的unsafe对象，NioMessageUnsafe。

1）先初始化好boss和worker的NioEventLoopGroup，并初始化好Group中的每一个NioEventLoop，为每一个NioEventLoop都穿件一个selector对象
2）Netty会在bind的动作上，去让boss的NioserverSocketChannel去绑定selector，并启动与boss捆绑在一起的thread，进入无尽的循环
3）在这个生命不息，循环不止的方法中，主要做了两件事情，1是去select，不管是selectNow()方法还是select()方法，其主要目的就是去查看boss关注的selector是否有事件发生
4）当有事件发生的时候，一般就是因为有client链接，如果有链接，boss线程就需要做的事情就是初始化封装好新来的SocketChannel
5）封装好的NioSocketChannel也会有自己的Unsafe对象，这个对象是用来做一些其他的操作的，例如读操作，这与boss的Unsafe对象不一样，boss的Unsafe对象是NioMessageUnsafe是用来进行绑定channel
6）NioSocketChannel用Boss线程管道中的ServerBootstrapAcceptor对象绑定确定属于自己的worker线程之后，绑定好worker线程的selector之后就开始调用自己的run方法
用来监听selector上的IO事件
7）要说明白的一点就是一个worker处理的channel在多链接的场景下，一个worker会处理不止一个SocketChannel



